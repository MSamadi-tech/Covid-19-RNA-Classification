{"cells":[{"cell_type":"code","execution_count":null,"id":"a90c5602-6a98-4b2c-88a1-7e43cd06c3f5","metadata":{"id":"a90c5602-6a98-4b2c-88a1-7e43cd06c3f5"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, Reshape, Conv2D,ReLU, Bidirectional\n","import pandas as pd\n","import numpy as np\n","import gc\n","import random"]},{"cell_type":"code","execution_count":null,"id":"4f929600-2604-42f8-a6cc-3e18c25940b9","metadata":{"id":"4f929600-2604-42f8-a6cc-3e18c25940b9"},"outputs":[],"source":["def clear_variable(var_list):\n","    for var in var_list:\n","        globals().pop(var, None)\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"id":"276a7da4-0a75-449c-8a5d-60424d2c06fa","metadata":{"id":"276a7da4-0a75-449c-8a5d-60424d2c06fa"},"outputs":[],"source":["subset_list = []\n","\n","for i in range(20):\n","  subset_list.append(f'Shuffled_Subset{i+1}.h5')"]},{"cell_type":"code","execution_count":null,"id":"cc81147b-e75d-4117-a3fe-f689ac70e0b1","metadata":{"id":"cc81147b-e75d-4117-a3fe-f689ac70e0b1"},"outputs":[],"source":["X_train_list, X_test_list, X_val_list = [], [], []\n","y_train_list, y_test_list, y_val_list = [], [], []\n","\n","for i in range(len(subset_list)):\n","  file_path = f'Shuffled_Subset{i+1}.h5'\n","  read_data = pd.read_hdf(file_path)\n","\n","  data_reshaped = np.array(read_data.drop('Class', axis=1)).reshape(read_data.shape[0],30900,4)\n","  data_labels = read_data['Class']\n","  clear_variable('read_data')\n","\n","  X_train_list.append(data_reshaped[:700])\n","  X_val_list.append(data_reshaped[700:800])\n","  X_test_list.append(data_reshaped[800:])\n","  clear_variable('data_reshaped')\n","\n","  y_train_list.append(data_labels[:700])\n","  y_val_list.append(data_labels[700:800])\n","  y_test_list.append(data_labels[800:])\n","  clear_variable('data_labels')\n","\n","X_train = np.concatenate(X_train_list, axis=0)\n","clear_variable('X_train_list')\n","\n","X_test = np.concatenate(X_test_list, axis=0)\n","clear_variable('X_test_list')\n","\n","X_val = np.concatenate(X_val_list, axis=0)\n","clear_variable('X_val_list')\n","\n","y_train = np.concatenate(y_train_list, axis=0)\n","clear_variable('y_train_list')\n","\n","y_test = np.concatenate(y_test_list, axis=0)\n","clear_variable('y_test_list')\n","\n","y_val = np.concatenate(y_val_list, axis=0)\n","clear_variable('y_val_list')"]},{"cell_type":"markdown","id":"a467fc43-9afc-4ab5-9816-915bfb353a80","metadata":{"id":"a467fc43-9afc-4ab5-9816-915bfb353a80"},"source":["## **Deep Learning Models**"]},{"cell_type":"code","execution_count":null,"id":"7a785c51-8137-46c1-b924-06196faf8698","metadata":{"id":"7a785c51-8137-46c1-b924-06196faf8698"},"outputs":[],"source":["num_classes = 8\n","input_shape = (30900, 4)"]},{"cell_type":"code","execution_count":null,"id":"67adc211-f9f1-4edb-97a1-e560ba66ad48","metadata":{"id":"67adc211-f9f1-4edb-97a1-e560ba66ad48"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.utils import Sequence\n","\n","class DataGenerator(Sequence):\n","    def __init__(self, X, y, batch_size):\n","        self.X = X\n","        self.y = y\n","        self.batch_size = batch_size\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.X) / self.batch_size))\n","\n","    def __getitem__(self, idx):\n","        batch_x = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        return np.array(batch_x), np.array(batch_y)\n"]},{"cell_type":"code","execution_count":null,"id":"dc0ef7c4-71bb-4f27-9acf-512ac42836e4","metadata":{"id":"dc0ef7c4-71bb-4f27-9acf-512ac42836e4"},"outputs":[],"source":["from tensorflow.keras.utils import Sequence\n","import numpy as np\n","\n","class CustomDataGenerator(Sequence):\n","    def __init__(self, data, labels, batch_size, shuffle=True):\n","        self.data = data\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(self.data))\n","\n","    def __len__(self):\n","        return int(np.floor(len(self.data) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        # Get the batch indexes\n","        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n","        batch_data = self.data[batch_indexes]\n","        batch_labels = self.labels[batch_indexes]\n","\n","        return batch_data, batch_labels\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n"]},{"cell_type":"code","execution_count":null,"id":"899b67e6-0c33-49d4-bb78-4e6d7178d5a0","metadata":{"id":"899b67e6-0c33-49d4-bb78-4e6d7178d5a0"},"outputs":[],"source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","def scheduler(epoch, lr):\n","    if epoch < 5:\n","        return lr\n","    else:\n","        return float(lr * tf.math.exp(-0.1).numpy())\n","\n","lr_scheduler = LearningRateScheduler(scheduler)\n"]},{"cell_type":"markdown","id":"6fb3a668-81a6-4673-a899-1de289ad7798","metadata":{"id":"6fb3a668-81a6-4673-a899-1de289ad7798"},"source":["### **CNN Model**"]},{"cell_type":"code","execution_count":null,"id":"4d418660-3472-42db-8120-3103da7ac0b6","metadata":{"id":"4d418660-3472-42db-8120-3103da7ac0b6"},"outputs":[],"source":["# Build the model\n","model = Sequential([\n","    Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape),\n","\n","    Conv2D(filters=32, kernel_size=(5, 5), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 4)),\n","\n","    Conv2D(filters=64, kernel_size=(5, 1), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 1)),\n","\n","    Conv2D(filters=128, kernel_size=(5, 1), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 1)),\n","\n","    Flatten(),\n","\n","    Dense(units=256, activation='relu'),\n","    Dropout(0.5),\n","\n","    Dense(units=num_classes, activation='softmax')\n","])\n","\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()\n"]},{"cell_type":"markdown","id":"6a1fe705-fb97-47d0-a088-a9c8f6608811","metadata":{"id":"6a1fe705-fb97-47d0-a088-a9c8f6608811"},"source":["### **CNN-LSTM Model**"]},{"cell_type":"code","execution_count":null,"id":"365ffc8f-62ff-4060-a9a1-b2c92caa202d","metadata":{"id":"365ffc8f-62ff-4060-a9a1-b2c92caa202d"},"outputs":[],"source":["# Build the model\n","model = Sequential([\n","    Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape),\n","\n","    Conv2D(filters=32, kernel_size=(5, 5), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 4)),\n","    Dropout(0.3),\n","\n","    Conv2D(filters=64, kernel_size=(5, 1), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 1)),\n","    Dropout(0.3),\n","\n","    Conv2D(filters=128, kernel_size=(5, 1), activation='relu', padding='same'),\n","    MaxPooling2D(pool_size=(4, 1)),\n","    Dropout(0.3),\n","\n","    Reshape((-1, 128)),\n","\n","    Bidirectional(LSTM(units=64, activation='tanh')),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(units=256, activation='relu'),\n","    Dropout(0.5),\n","\n","    Dense(units=num_classes, activation='softmax')\n","])\n","\n","\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"id":"e0439105-ce47-4d3d-a62a-a939a986d143","metadata":{"id":"e0439105-ce47-4d3d-a62a-a939a986d143"},"outputs":[],"source":["# Assuming data is split into train, val, test sets\n","train_gen = DataGenerator(X_train, y_train, batch_size=32)\n","val_gen = DataGenerator(X_val, y_val, batch_size=32)\n","\n","# Now train the model\n","history = model.fit(\n","    train_gen,\n","    validation_data=val_gen,\n","    epochs=25,\n","    callbacks=[lr_scheduler],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":null,"id":"2fc3bc00-96a8-49c8-86e7-e2825086691c","metadata":{"id":"2fc3bc00-96a8-49c8-86e7-e2825086691c"},"outputs":[],"source":["test_gen = DataGenerator(X_test, y_test, batch_size=64)\n","test_loss, test_acc = model.evaluate(test_gen)\n","\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")"]},{"cell_type":"code","execution_count":null,"id":"6fab3d0d-2d5f-465e-8c31-62f16974ce10","metadata":{"id":"6fab3d0d-2d5f-465e-8c31-62f16974ce10"},"outputs":[],"source":["# Save the related model\n","model.save('models/one-hot_cnn_lstm.keras')\n","model.save('models/one-hot_cnn.keras')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}