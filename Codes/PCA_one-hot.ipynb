{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa9Mn7pWMBD9SvM5wCp5GG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import gc\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import PCA, IncrementalPCA"
      ],
      "metadata": {
        "id": "YP5WkniOfPFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "udCp5HEnfRw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_variable(var_list):\n",
        "    for var in var_list:\n",
        "        globals().pop(var, None)  # Remove from global scope\n",
        "    gc.collect() # Run garbage collection"
      ],
      "metadata": {
        "id": "T1eXRPpcfSPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = []\n",
        "\n",
        "for i in range(20):\n",
        "  file_list.append(f'Shuffled_Subset{i}.h5')"
      ],
      "metadata": {
        "id": "V_OtoswZfT1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inc_pca_32 = IncrementalPCA(n_components=32)\n",
        "inc_pca_64 = IncrementalPCA(n_components=64)\n",
        "inc_pca_128 = IncrementalPCA(n_components=128)\n",
        "inc_pca_256 = IncrementalPCA(n_components=256)"
      ],
      "metadata": {
        "id": "9Sr4l2oJfWLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(file_list)):\n",
        "  file_path = '/content/drive/MyDrive/ML_DL_Datasets/DNA_Datasets/Shuffled_Datasets/Covid_Shuffled_Balanced/Shuffled_Subset{file_counter}.h5'.format(file_counter = i+1)\n",
        "  read_data = pd.read_hdf(file_path) # Read the current dataset\n",
        "\n",
        "  data_reshaped = np.array(read_data.drop('Class', axis=1))\n",
        "  clear_variable('read_data')\n",
        "\n",
        "  inc_pca_train = data_reshaped[:800]\n",
        "  clear_variable('data_reshaped')\n",
        "\n",
        "  inc_pca_32.fit(inc_pca_train) # Fit Inc_PCA\n",
        "  inc_pca_64.fit(inc_pca_train) # Fit Inc_PCA\n",
        "  inc_pca_128.fit(inc_pca_train) # Fit Inc_PCA\n",
        "  inc_pca_256.fit(inc_pca_train) # Fit Inc_PCA\n",
        "  clear_variable('inc_pca_train')"
      ],
      "metadata": {
        "id": "igZ0_WUzfkz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the IncrementalPCA object to a file\n",
        "joblib.dump(inc_pca_32, '/content/drive/MyDrive/ML_Models/Inc_PCA_n32_Covid.pkl')  # Save to a pickle file\n",
        "joblib.dump(inc_pca_64, '/content/drive/MyDrive/ML_Models/Inc_PCA_n64_Covid.pkl')  # Save to a pickle file\n",
        "joblib.dump(inc_pca_128, '/content/drive/MyDrive/ML_Models/Inc_PCA_n128_Covid.pkl')  # Save to a pickle file\n",
        "joblib.dump(inc_pca_256, '/content/drive/MyDrive/ML_Models/Inc_PCA_n256_Covid.pkl')  # Save to a pickle file"
      ],
      "metadata": {
        "id": "3ncKfE40fzB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
