{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNowjvIZIxz6HrWPe2UKflx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CtLdI4b_ZVE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import gc\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4XD1fEo_qMt",
        "outputId": "92f951c2-719f-4f88-bb4b-1e7498689515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_fasta(file_path):\n",
        "    sequences = {}\n",
        "    current_sequence = ''\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                if current_sequence:\n",
        "                    sequences[header] = current_sequence\n",
        "                    current_sequence = ''\n",
        "                header = line[1:]\n",
        "            else:\n",
        "                current_sequence += line\n",
        "        if current_sequence:\n",
        "            sequences[header] = current_sequence\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "I7d4Q8ma_qj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_variable(var_list):\n",
        "    for var in var_list:\n",
        "        globals().pop(var, None)  # Remove from global scope\n",
        "    gc.collect() # Run garbage collection"
      ],
      "metadata": {
        "id": "4Q2Dcvft_sNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_data_size(file_list):\n",
        "  all_data_size = 0\n",
        "  for file in file_list:\n",
        "    all_data_size += int(file.split('-')[-1].split('.')[0]) # Adding every dataset's size together\n",
        "  return all_data_size"
      ],
      "metadata": {
        "id": "w-8A-iah_uC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nuc_to_df_nuc(data_nuc, num_cols):\n",
        "  sep_sequences = []\n",
        "\n",
        "  for seq in data_nuc:\n",
        "      tmp_char = []\n",
        "      for char in seq:\n",
        "          tmp_char.append(char)\n",
        "      sep_sequences.append(tmp_char)\n",
        "\n",
        "  tmp_df = pd.DataFrame(sep_sequences)\n",
        "  clear_variable(['sep_sequences']) # Clean up memory\n",
        "\n",
        "  fixed_df = pd.DataFrame(np.nan, index=range(len(data_nuc)), columns=range(num_cols))\n",
        "  fixed_df.iloc[:len(tmp_df), :tmp_df.shape[1]] = tmp_df\n",
        "  clear_variable(['tmp_df']) # Clean up memory\n",
        "  return fixed_df"
      ],
      "metadata": {
        "id": "UYlYPBAa_2cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(samples_df):\n",
        "    tmp_data = []  # Initialize a list to hold data for DataFrame creation\n",
        "    for i, row in samples_df.iterrows():  # Iterate over the rows of samples_df\n",
        "        row_data = {}  # Initialize a dictionary to hold data for the current row\n",
        "        for j, char in enumerate(row):  # Iterate over the characters in each row\n",
        "            if char == 'A':\n",
        "                row_data[f'Nuc_A{j}'], row_data[f'Nuc_T{j}'], row_data[f'Nuc_C{j}'], row_data[f'Nuc_G{j}'] = 1, 0, 0, 0\n",
        "            elif char == 'T':\n",
        "                row_data[f'Nuc_A{j}'], row_data[f'Nuc_T{j}'], row_data[f'Nuc_C{j}'], row_data[f'Nuc_G{j}'] = 0, 1, 0, 0\n",
        "            elif char == 'C':\n",
        "                row_data[f'Nuc_A{j}'], row_data[f'Nuc_T{j}'], row_data[f'Nuc_C{j}'], row_data[f'Nuc_G{j}'] = 0, 0, 1, 0\n",
        "            elif char == 'G':\n",
        "                row_data[f'Nuc_A{j}'], row_data[f'Nuc_T{j}'], row_data[f'Nuc_C{j}'], row_data[f'Nuc_G{j}'] = 0, 0, 0, 1\n",
        "            else:\n",
        "                row_data[f'Nuc_A{j}'], row_data[f'Nuc_T{j}'], row_data[f'Nuc_C{j}'], row_data[f'Nuc_G{j}'] = 0, 0, 0, 0\n",
        "        tmp_data.append(row_data)  # Append the data for the current row to the list\n",
        "\n",
        "    return pd.DataFrame(tmp_data)  # Create the DataFrame from the list of dictionaries and return it"
      ],
      "metadata": {
        "id": "9gNkhYij_4H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = [\n",
        "    'Cov-Alpha-US-13207.fasta',\n",
        "    'Cov-BA.2.12.1-usa-11331.fasta',\n",
        "    'Cov-Delta-US-10117.fasta',\n",
        "    'Cov-BQ.1.1-usa-9999.fasta',\n",
        "    'Cov-BA.1.1-usa-6694.fasta',\n",
        "    'Cov-Gama-US-4995.fasta',\n",
        "    'Cov-BA.5.4-3631.fasta',\n",
        "    'Cov-BA.4.6-2607.fasta'\n",
        "]"
      ],
      "metadata": {
        "id": "38XoZM6M_6RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the size of all datasets\n",
        "subset_size = 1000\n",
        "num_cols = 30900\n",
        "num_classes = 8\n",
        "\n",
        "# Calculate the proportion of each class in the subset\n",
        "proportion = int(subset_size / num_classes)\n",
        "\n",
        "all_data_size = find_all_data_size(file_list)\n",
        "n_subsets = math.ceil(all_data_size / subset_size)\n",
        "\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "  # Read each dataset\n",
        "  file_path = '/content/drive/MyDrive/ML_DL_Datasets/DNA_Datasets/{path}'.format(path=file_list[i])\n",
        "  data_nuc = [] # Intermediate list to store nucleotide sequences\n",
        "  data = read_fasta(file_path) # Read the current dataset\n",
        "\n",
        "  # Process the data to extract nucleotide sequences\n",
        "  for key in data.keys():\n",
        "      data_nuc.append(data[key])\n",
        "  clear_variable(['data']) # Clean up memory\n",
        "  random.seed(101)\n",
        "  random.shuffle(data_nuc) # shuffling data\n",
        "\n",
        "  # Other operations done on each batches of data\n",
        "  file_counter = 1\n",
        "  for start in range(0, 2500, proportion):\n",
        "      batch_data_nuc = data_nuc[start : start + proportion]  # Create a smaller data_nuc for the batch\n",
        "\n",
        "      # convert nucleotide list to the dataframe\n",
        "      df_nuc = nuc_to_df_nuc(batch_data_nuc, num_cols)\n",
        "      clear_variable(['batch_data_nuc']) # Clean up memory\n",
        "\n",
        "      # Fill the empty cells\n",
        "      df_nuc.fillna(value = 0, inplace=True)  # Fill the empty cells\n",
        "\n",
        "      # One-hot encode\n",
        "      one_hot_encoded_df = one_hot_encode(df_nuc)  # Convert the nucleotide dataframe to numerical foramt with one-hot encoding\n",
        "      clear_variable(['df_nuc']) # Clean up memory\n",
        "\n",
        "      # Add Class\n",
        "      one_hot_encoded_df['Class'] = np.ones(len(one_hot_encoded_df))  # Assign a default class\n",
        "      one_hot_encoded_df['Class'] = one_hot_encoded_df['Class'].apply(lambda x: x * i) # Adjust 'Class' for identification\n",
        "\n",
        "      # Change the datatype\n",
        "      final_df = one_hot_encoded_df.astype('uint8')\n",
        "      clear_variable(['one_hot_encoded_df']) # Clean up memory\n",
        "\n",
        "      # Save the data\n",
        "      final_df.to_hdf('/content/drive/MyDrive/Datasets/temporary/class{i}_subset{file_counter}.h5'.format(i = i, file_counter = file_counter), key='data')\n",
        "      clear_variable(['final_df']) # Clean up memory\n",
        "      file_counter += 1\n",
        "\n",
        "  clear_variable(['data_nuc']) # Clean up memory\n",
        "\n",
        "# Read each subset, concatenate and shuffle them\n",
        "for i in range(n_subsets):\n",
        "  subsets_tmp = []\n",
        "  for j in range(len(file_list)):\n",
        "    tmp_subset = pd.read_hdf('/content/drive/MyDrive/Datasets/temporary/class{j}_subset{file_counter}.h5'.format(j = j, file_counter = i+1))\n",
        "    subsets_tmp.append(tmp_subset)\n",
        "    clear_variable(['tmp_subset']) # Clean up memory\n",
        "\n",
        "  subset = pd.concat(subsets_tmp)\n",
        "  clear_variable(['subsets_tmp']) # Clean up memory\n",
        "\n",
        "  # Shuffle the subset\n",
        "  shuffled_subset = subset.sample(frac=1, random_state=101).reset_index(drop=True)\n",
        "  clear_variable(['subset']) # Clean up memory\n",
        "\n",
        "  shuffled_subset.to_hdf('/content/drive/MyDrive/Datasets/Covid_Shuffled_Balanced_Train_Test/Shuffled_Subset{file_counter}.h5'.format(file_counter = i+1), key='data')\n",
        "  clear_variable(['shuffled_subset']) # Clean up memory"
      ],
      "metadata": {
        "id": "7I2o2LUI_850",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Uv0HajWDZa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}