{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkhFQYejYo1jWDa9vSWSre"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIPToZKXastT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import gc\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Gx0FkymabtRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_variable(var_list):\n",
        "    for var in var_list:\n",
        "        globals().pop(var, None)  # Remove from global scope\n",
        "    gc.collect() # Run garbage collection"
      ],
      "metadata": {
        "id": "9RfCwOlbbtx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = []\n",
        "\n",
        "for i in range(20):\n",
        "  file_list.append(f'Shuffled_Subset{i+1}.h5')"
      ],
      "metadata": {
        "id": "36RipZYxbwbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_list, X_test_list, X_val_list = [], [], []\n",
        "y_train_list, y_test_list, y_val_list = [], [], []\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "  file_path = '/content/drive/MyDrive/ML_DL_Datasets/DNA_Datasets/Shuffled_Datasets/Covid_Shuffled_Balanced/Shuffled_Subset{file_counter}.h5'.format(file_counter = i+1)\n",
        "  read_data = pd.read_hdf(file_path) # Read the current dataset\n",
        "\n",
        "  data_reshaped = np.array(read_data.drop('Class', axis=1)).reshape(read_data.shape[0],30900,4)\n",
        "  data_labels = read_data['Class']\n",
        "  clear_variable('read_data')\n",
        "\n",
        "  X_train_list.append(data_reshaped[:700])\n",
        "  X_val_list.append(data_reshaped[700:800])\n",
        "  X_test_list.append(data_reshaped[800:])\n",
        "  clear_variable('data_reshaped')\n",
        "\n",
        "  y_train_list.append(data_labels[:700])\n",
        "  y_val_list.append(data_labels[700:800])\n",
        "  y_test_list.append(data_labels[800:])\n",
        "  clear_variable('data_labels')\n",
        "\n",
        "X_train = np.concatenate(X_train_list, axis=0)\n",
        "clear_variable('X_train_list')\n",
        "\n",
        "X_test = np.concatenate(X_test_list, axis=0)\n",
        "clear_variable('X_test_list')\n",
        "\n",
        "X_val = np.concatenate(X_val_list, axis=0)\n",
        "clear_variable('X_val_list')\n",
        "\n",
        "y_train = np.concatenate(y_train_list, axis=0)\n",
        "clear_variable('y_train_list')\n",
        "\n",
        "y_test = np.concatenate(y_test_list, axis=0)\n",
        "clear_variable('y_test_list')\n",
        "\n",
        "y_val = np.concatenate(y_val_list, axis=0)\n",
        "clear_variable('y_val_list')"
      ],
      "metadata": {
        "id": "QfG39KzebyBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Reshape input to 3D\n",
        "    model.add(layers.Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape))\n",
        "\n",
        "    # Convolutional Layers\n",
        "    model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 1)))  # Adjusted pool size\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense Layers\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(8, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "azin-XTDbzOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate scheduler callback\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch > 30 and epoch <= 40:\n",
        "        return 0.0001\n",
        "    elif epoch > 40:\n",
        "        return 0.00001\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (30900, 4)  # Shape of the input data: 30900 sequences, each with 4 features\n",
        "num_classes = 8  # Number of classes for classification\n",
        "\n",
        "# Create the CNN model\n",
        "cnn_model = create_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "1cBPf4Kecipp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = cnn_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "id": "35fjGOhzctlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = cnn_model_2.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Z_G2AWcdczy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the model\n",
        "predictions = cnn_model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute and format the classification report\n",
        "class_report = classification_report(y_test, predicted_labels, output_dict=True)\n",
        "class_report_df = pd.DataFrame(class_report).transpose()\n",
        "class_report_df = class_report_df.round(4)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(class_report_df.to_string())"
      ],
      "metadata": {
        "id": "JfdGMVaqc5VL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}